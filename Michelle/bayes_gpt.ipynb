import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, f1_score

# Lese die Daten ein
col_names = ['contour number', 'aspect ratio', 'extent', 'Blue', 'Green', 'Red', 'Hue', 'class']
data = pd.read_csv("output.csv", skiprows=1, header=None, names=col_names)

# Teile die Daten in Trainings- und Testdaten auf
train, test = train_test_split(data, test_size=.2, random_state=41)

def calculate_prior(df, Y):
    classes, counts = np.unique(df[Y], return_counts=True)
    prior = counts / len(df)
    return prior

def calculate_likelihood_gaussian(df, feat_name, feat_val, Y, label):
    df_class = df[df[Y] == label]
    mean, std = df_class[feat_name].mean(), df_class[feat_name].std()
    p_x_given_y = (1 / (np.sqrt(2 * np.pi) * std)) * np.exp(-((feat_val - mean) ** 2) / (2 * std ** 2))
    return p_x_given_y

def naive_bayes_gaussian(df, X, Y):
    prior = calculate_prior(df, Y)
    labels = np.unique(df[Y])
    likelihood = np.zeros((len(labels), len(X[0])))

    for j, label in enumerate(labels):
        for i, feat_name in enumerate(df.columns[:-1]):
            likelihood[j, i] = calculate_likelihood_gaussian(df, feat_name, X[0, i], Y, label)

    post_prob = prior * np.prod(likelihood, axis=1)
    Y_pred = np.argmax(post_prob)
    return Y_pred

# Teste das Modell
X_test = test.iloc[:, :-1].values
Y_test = test.iloc[:, -1].values

Y_pred = np.apply_along_axis(naive_bayes_gaussian, 1, X_test, train, "class")

# Evaluierung
print(confusion_matrix(Y_test, Y_pred))
print(f1_score(Y_test, Y_pred, average='weighted'))
